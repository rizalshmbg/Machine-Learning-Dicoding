{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5c. Binary Text Classification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM4nctZvS6yV3+OBF5XunJb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FnDH2EeRiUlF"},"source":["Sebelumnya kita telah belajar bagaimana melakukan tokenisasi dan membuat sequence dari setiap kalimat pada teks. Bayangkan jika Anda memiliki restoran dan Anda memiliki ratusan ribu review pelanggan. Anda menginginkan agar membedakan antara review positif dan negatif menjadi terotomasi sehingga Anda dapat memfilter review pelanggan dengan mudah. Pada submodul ini kita akan latihan untuk masalah ini. \n","\n","Untuk latihan kali ini kita akan menggunakan dataset **Yelp** yang berisi review dari beberapa restoran di Amerika Serikat. Dataset terdiri dari 2 kelas yaitu 0 dan 1 yang menunjukkan apakah review tersebut positif atau negatif. Dataset dapat diunduh pada [tautan](https://www.kaggle.com/marklvl/sentiment-labelled-sentences-data-setps://) ini yah. "]},{"cell_type":"markdown","metadata":{"id":"uFwF_vX1i7ru"},"source":["Setelah dataset diunduh, kita load dataset pada Colaboratory. Dan pastinya kita buat dataframe dari dataset kita agar lebih mudah diproses."]},{"cell_type":"code","metadata":{"id":"om2MZ4l8iS7H","executionInfo":{"status":"ok","timestamp":1632320364443,"user_tz":-420,"elapsed":400,"user":{"displayName":"Rizal Sihombing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHdaFgVUUc0oj1CRMgNMsZ6dVCXXiXl88lNlBLDw=s64","userId":"15325755837778064720"}}},"source":["import pandas as pd\n","\n","df = pd.read_csv('/content/yelp_labelled.txt', names=['sentences', 'label'], sep='\\t')"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AWk-qs6QjWeW"},"source":["Untuk melihat 5 sampel terakhir dari dataset kita, panggil fungsi `tail()`. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"14Vaj-_CjYrV","executionInfo":{"status":"ok","timestamp":1632320369684,"user_tz":-420,"elapsed":413,"user":{"displayName":"Rizal Sihombing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHdaFgVUUc0oj1CRMgNMsZ6dVCXXiXl88lNlBLDw=s64","userId":"15325755837778064720"}},"outputId":"c607cba7-3c1b-4b76-e7d5-f25a46cad8f2"},"source":["df.tail()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentences</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>995</th>\n","      <td>I think food should have flavor and texture an...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>Appetite instantly gone.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>Overall I was not impressed and would not go b...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>The whole experience was underwhelming, and I ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>Then, as if I hadn't wasted enough of my life ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             sentences  label\n","995  I think food should have flavor and texture an...      0\n","996                           Appetite instantly gone.      0\n","997  Overall I was not impressed and would not go b...      0\n","998  The whole experience was underwhelming, and I ...      0\n","999  Then, as if I hadn't wasted enough of my life ...      0"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"UsXAkv7ljanG"},"source":["Setelah itu kita bagi dataset menjadi 'train' dan 'test' set."]},{"cell_type":"code","metadata":{"id":"z7FGKJ06jeai"},"source":["from sklearn.model_selection import train_test_split\n","\n","kalimat = df['sentences'].values\n","y = df['label'].values\n","kalimat_latih, kalimat_test, y_latih, y_test = train_test_split(kalimat, y, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W9AbrJrIj5KI"},"source":["Agar teks dapat dipahami oleh model, kita harus lakukan tokenisasi. Gunakan fungsi `tokenizer` pada data latih dan data test. Jangan lupa gunakan fungsi `pad_sequences` agar setiap sequence sama panjang."]},{"cell_type":"code","metadata":{"id":"gK40UuF2j7yU"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","tokenizer = Tokenizer(num_words=250, oov_token='x')\n","tokenizer.fit_on_texts(kalimat_latih)\n","tokenizer.fit_on_texts(kalimat_test)\n","\n","sekuens_latih = tokenizer.texts_to_sequences(kalimat_latih)\n","sekuens_test = tokenizer.texts_to_sequences(kalimat_test)\n","\n","padded_latih = pad_sequences(sekuens_latih)\n","padded_test = pad_sequences(sekuens_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SVp2SU6UpbNn"},"source":["Untuk arsitektur yang akan digunakan adalah *layer embedding*, dengan argumen pertama sesuai dengan jumlah vocabulary/kata yang kita pakai pada tokenizer. Argumen selanjutnya adalah *dimensi embedding*, dan *input_length* yang merupakan panjang dari sequence. Nah di kita tidak menggunakan layer *Flatten* melainkan kita menggantinya dengan **GlobalAveragePooling1D**. Fungsi ini bekerja lebih baik pada kasus NLP dibanding *Flatten*."]},{"cell_type":"code","metadata":{"id":"26RPPpSKpmKZ"},"source":["import tensorflow as tf\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(250, 16, input_length=20),\n","    tf.keras.layers.GlobalAveragePooling1D(),\n","    tf.keras.layers.Dense(24, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid'),\n","])\n","model.compile(\n","    loss='binary_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy']\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6NzccLDDqLuZ"},"source":["Setelah arsitektur model dibentuk, dan loss function serta optimizer ditentukan, kita dapat memulai pelatihan model kita. Di sini penulis menggunakan 30 epoch. Anda bebas bereksperimen dengan nilai yang lain."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eq0NzpRAqOxA","executionInfo":{"status":"ok","timestamp":1632283323553,"user_tz":-420,"elapsed":2762,"user":{"displayName":"Rizal Sihombing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHdaFgVUUc0oj1CRMgNMsZ6dVCXXiXl88lNlBLDw=s64","userId":"15325755837778064720"}},"outputId":"97ddc319-5264-42cd-af3f-8f6d4595f919"},"source":["num_epochs = 30\n","history = model.fit(padded_latih, y_latih, epochs=num_epochs,\n","                    validation_data=(padded_test, y_test),\n","                    verbose=2)\n","\n","# Hasil dari pelatihan model kita menunjukkan akurasi yang cukup baik dengan data yang sangat sedikit."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","25/25 - 0s - loss: 0.1975 - accuracy: 0.9262 - val_loss: 0.8680 - val_accuracy: 0.7000\n","Epoch 2/30\n","25/25 - 0s - loss: 0.1932 - accuracy: 0.9262 - val_loss: 0.8343 - val_accuracy: 0.6900\n","Epoch 3/30\n","25/25 - 0s - loss: 0.1901 - accuracy: 0.9275 - val_loss: 0.8379 - val_accuracy: 0.6950\n","Epoch 4/30\n","25/25 - 0s - loss: 0.1912 - accuracy: 0.9250 - val_loss: 0.8583 - val_accuracy: 0.6950\n","Epoch 5/30\n","25/25 - 0s - loss: 0.1930 - accuracy: 0.9262 - val_loss: 0.8530 - val_accuracy: 0.6850\n","Epoch 6/30\n","25/25 - 0s - loss: 0.1903 - accuracy: 0.9275 - val_loss: 0.8672 - val_accuracy: 0.7000\n","Epoch 7/30\n","25/25 - 0s - loss: 0.1877 - accuracy: 0.9337 - val_loss: 0.8692 - val_accuracy: 0.6850\n","Epoch 8/30\n","25/25 - 0s - loss: 0.1871 - accuracy: 0.9325 - val_loss: 0.8786 - val_accuracy: 0.6900\n","Epoch 9/30\n","25/25 - 0s - loss: 0.1877 - accuracy: 0.9287 - val_loss: 0.9155 - val_accuracy: 0.6950\n","Epoch 10/30\n","25/25 - 0s - loss: 0.1858 - accuracy: 0.9300 - val_loss: 0.8867 - val_accuracy: 0.6800\n","Epoch 11/30\n","25/25 - 0s - loss: 0.1842 - accuracy: 0.9275 - val_loss: 0.9091 - val_accuracy: 0.7000\n","Epoch 12/30\n","25/25 - 0s - loss: 0.1854 - accuracy: 0.9325 - val_loss: 0.9165 - val_accuracy: 0.7000\n","Epoch 13/30\n","25/25 - 0s - loss: 0.1851 - accuracy: 0.9362 - val_loss: 0.9088 - val_accuracy: 0.6850\n","Epoch 14/30\n","25/25 - 0s - loss: 0.1844 - accuracy: 0.9350 - val_loss: 0.9125 - val_accuracy: 0.6800\n","Epoch 15/30\n","25/25 - 0s - loss: 0.1872 - accuracy: 0.9300 - val_loss: 0.9194 - val_accuracy: 0.6800\n","Epoch 16/30\n","25/25 - 0s - loss: 0.1842 - accuracy: 0.9362 - val_loss: 0.9305 - val_accuracy: 0.6900\n","Epoch 17/30\n","25/25 - 0s - loss: 0.1818 - accuracy: 0.9312 - val_loss: 0.9429 - val_accuracy: 0.6950\n","Epoch 18/30\n","25/25 - 0s - loss: 0.1804 - accuracy: 0.9362 - val_loss: 0.9340 - val_accuracy: 0.6800\n","Epoch 19/30\n","25/25 - 0s - loss: 0.1808 - accuracy: 0.9362 - val_loss: 0.9392 - val_accuracy: 0.6950\n","Epoch 20/30\n","25/25 - 0s - loss: 0.1825 - accuracy: 0.9312 - val_loss: 0.9563 - val_accuracy: 0.6850\n","Epoch 21/30\n","25/25 - 0s - loss: 0.1791 - accuracy: 0.9262 - val_loss: 0.9643 - val_accuracy: 0.6900\n","Epoch 22/30\n","25/25 - 0s - loss: 0.1831 - accuracy: 0.9325 - val_loss: 0.9587 - val_accuracy: 0.6900\n","Epoch 23/30\n","25/25 - 0s - loss: 0.1784 - accuracy: 0.9388 - val_loss: 0.9766 - val_accuracy: 0.6900\n","Epoch 24/30\n","25/25 - 0s - loss: 0.1768 - accuracy: 0.9362 - val_loss: 0.9914 - val_accuracy: 0.6950\n","Epoch 25/30\n","25/25 - 0s - loss: 0.1772 - accuracy: 0.9375 - val_loss: 0.9861 - val_accuracy: 0.6950\n","Epoch 26/30\n","25/25 - 0s - loss: 0.1767 - accuracy: 0.9375 - val_loss: 1.0044 - val_accuracy: 0.6950\n","Epoch 27/30\n","25/25 - 0s - loss: 0.1775 - accuracy: 0.9325 - val_loss: 0.9996 - val_accuracy: 0.6900\n","Epoch 28/30\n","25/25 - 0s - loss: 0.1808 - accuracy: 0.9262 - val_loss: 0.9831 - val_accuracy: 0.7000\n","Epoch 29/30\n","25/25 - 0s - loss: 0.1776 - accuracy: 0.9362 - val_loss: 1.0143 - val_accuracy: 0.6950\n","Epoch 30/30\n","25/25 - 0s - loss: 0.1748 - accuracy: 0.9337 - val_loss: 0.9984 - val_accuracy: 0.7000\n"]}]},{"cell_type":"markdown","metadata":{"id":"qBObOBooqnA9"},"source":["# Bagaimana? mudah bukan membuat model untuk klasifikasi teks. DI submodul selanjutnya kita akan belajar menggunakan layer LSTM dalam kasus NLP."]}]}